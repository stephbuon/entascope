{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import re\n",
    "import pandas as pd \n",
    "import spacy\n",
    "from datetime import date\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'lemmatizer', 'tagger'])\n",
    "\n",
    "# if i scrape NPR The head needs to be on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from selenium.webdriver.support.ui import Select\n",
    "\n",
    "#def search_archive(driver):\n",
    "\n",
    "#    select = Select(driver.find_element(By.CSS_SELECTOR, 'div.month:nth-child(2) > ul:nth-child(2)'))\n",
    "    \n",
    "#    select.select_by_value('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i need a make dir function     if not os.path.exists(/home/stephbuon/projects/entascope/results/ + ts):\n",
    "        #os.mkdir(export_path + ts)\n",
    "\n",
    "def import_existing_citations(existing_set, source):\n",
    "    if existing_set == True:\n",
    "        scraped_pages_df = pd.read_csv('/home/stephbuon/projects/entascope/data/scraped_pages/' + ts + '/' + 'citations_' + source + '_' + ts + '.csv')\n",
    "        scraped_pages_df['url'] = scraped_pages_df['url'].apply(lambda x: x.split('/')[-1])\n",
    "        scraped_pages = dict(zip(scraped_pages_df['url'], scraped_pages_df['source']))\n",
    "        \n",
    "        return scraped_pages # do max_no elsewhere \n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def start_selenium(source, url):\n",
    "    options = Options()\n",
    "    #options.headless = True\n",
    "    driver = webdriver.Firefox(executable_path='/home/stephbuon/projects/entascope/geckodriver', options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    if source == 'FOX':\n",
    "        #search_archive(driver)\n",
    "        \n",
    "        #WebDriverWait(driver, 1).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.load-more > a:nth-child(1)'))).click() # get rid of this for media -- or maybe i Have no need at all \n",
    "        for i in range(100):\n",
    "            try:\n",
    "                driver.find_element(By.CSS_SELECTOR, '.pf-widget-close').click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.load-more > a:nth-child(1)'))))\n",
    "                WebDriverWait(driver, 1).until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.load-more > a:nth-child(1)'))).click()\n",
    "            except Exception as e2: \n",
    "                print(e2)\n",
    "                print('click number ' + str(i))\n",
    "                pass\n",
    "\n",
    "        return driver.page_source\n",
    "\n",
    "    if source == 'NPR':\n",
    "        scroll_number = 5\n",
    "        for i in range(1, scroll_number):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                driver.find_element(By.CSS_SELECTOR, '.global-modal__dismiss').click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return driver.page_source\n",
    "\n",
    "    if source == 'MSNBC':\n",
    "        return driver.page_source\n",
    "\n",
    "\n",
    "def validate_url(url, page_url, source):\n",
    "    if source == 'FOX':\n",
    "        if not 'https' in page_url:\n",
    "            if '/politics/' in page_url or '/media/' in page_url or '/us/':\n",
    "                page_url = 'https://www.foxnews.com' + page_url            # make sure there are no doubles \n",
    "                try:\n",
    "                    return requests.get(page_url)\n",
    "                except Exception as e3:\n",
    "                    print(e3)\n",
    "                    return None\n",
    "                    \n",
    "        elif 'q=coronavirus' in url:\n",
    "            return requests.get(page_url)\n",
    "    \n",
    "    elif source == 'NPR':\n",
    "        #if not 'ondemand.npr.org' and 'mp3?' in page_url:\n",
    "        try:\n",
    "            return requests.get(page_url)\n",
    "        except Exception as e4:\n",
    "            print(e4)\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            return requests.get(page_url)\n",
    "        except Exception as e4:\n",
    "                    print(e4)\n",
    "                    return None\n",
    "\n",
    "\n",
    "def export_data(export_path, scraped_pages, out, source, ts):\n",
    "\n",
    "    if not os.path.exists(export_path + ts):\n",
    "        os.mkdir(export_path + ts)\n",
    "\n",
    "    citations = pd.DataFrame(\n",
    "    {'url': scraped_pages.keys(), 'source': scraped_pages.values()} )\n",
    "\n",
    "    clean_stuff = pd.DataFrame(citations['source'].tolist(), columns=['source', 'article_id'])\n",
    "\n",
    "    del(citations['source'])\n",
    "    citations = pd.concat([citations, clean_stuff], axis=1)\n",
    "\n",
    "    citations.to_csv(export_path + ts + '/' + 'citations_' + source + '_' + ts + '.csv', index = False)\n",
    "\n",
    "\n",
    "    named_entites = pd.DataFrame(\n",
    "        {'entity_label': out.keys(), 'entity_text': out.values() } )\n",
    "            \n",
    "    named_entites = named_entites.explode('entity_text')\n",
    "\n",
    "    if not os.path.isfile(export_path + ts + '/' + 'named_entities_' + source + '_' + ts + '.csv'):\n",
    "        named_entites.to_csv(export_path + ts + '/' + 'named_entities_' + source + '_' + ts + '.csv')\n",
    "    else:\n",
    "        old_named_entities = pd.read_csv(export_path + ts + '/' + 'named_entities_' + source + '_' + ts + '.csv')\n",
    "        named_entites = pd.concat([named_entites, old_named_entities], ignore_index = True)\n",
    "\n",
    "        named_entites.to_csv(export_path + ts + '/' + 'named_entities_' + source + '_' + ts + '.csv', index = False)\n",
    "\n",
    "\n",
    "# NORP\n",
    "#normberg law \n",
    "def show_entity_labels():\n",
    "    print(\"\"\"\n",
    "    PERSON:        People's names\n",
    "    NORP:          Nationalities or religiouse or political groups.\n",
    "    FAC:           Infrastructure such as buildings, airports, highways, bridges, etc.\n",
    "    ORG:           Companies, agencies, institutions, etc.\n",
    "    GPE:           Countries, cities, states.\n",
    "    LOC:           Non-GPE locations, mountain ranges, bodies of water. \n",
    "    PRODUCT:       Objects, vehicles, foods, etc. (Not services.)\n",
    "    EVENT:         Names of events. Natural disasters, battles, sports events, etc. Coronavirus strains like \"omicron\" are categorized as events.\n",
    "    WORK_OF_ART:   Titles of books, songs, etc.\n",
    "    LAW:           Names of documents made into laws.\n",
    "    LANGUAGE:      Named languages.\n",
    "    DATE:          Absolute or relative dates or periods.\n",
    "    TIME:          Times smaller than a day.\n",
    "    PERCENT:       Percentage, including \"%\".\n",
    "    MONEY:         Monetary values.\n",
    "    QUANTITY:      Measurements, as of weight or distance.\n",
    "    ORDINAL:       Terms denoting order, such as \"first\" or \"second.\"\n",
    "    CARDINAL:      Numerals that do not fall under another time. \n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_named_entities(requests_url):\n",
    "    ne_dictionary = {}\n",
    "\n",
    "    if requests_url is not None:\n",
    "        try:\n",
    "            page_content = BeautifulSoup(requests_url.text, 'html.parser')\n",
    "            \n",
    "            for paragraph in page_content.findAll('p'):\n",
    "                paragraph = list(paragraph.stripped_strings)\n",
    "                \n",
    "                for sentence in paragraph:\n",
    "                    sentence = nlp(sentence)\n",
    "                    \n",
    "                    for entity in sentence.ents:\n",
    "                        if not entity.label_ in ne_dictionary.keys():\n",
    "                            ne_dictionary[entity.label_] = [entity.text]\n",
    "                        else:\n",
    "                            ne_dictionary[entity.label_].append(entity.text)\n",
    "        \n",
    "        except Exception as e5:\n",
    "            print(e5)\n",
    "    \n",
    "    return ne_dictionary\n",
    "\n",
    "\n",
    "\n",
    "def time_stamp():\n",
    "    today = date.today()\n",
    "    d_today = today.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "    return d_today\n",
    "\n",
    "\n",
    "def scraper(url, response, source, ts, **kwargs):\n",
    "\n",
    "    keyword = re.compile(r'covid|coronavirus|vaccine|omicron|delta|vaccinat|vaxx|pandemic|mask', re.IGNORECASE)\n",
    "\n",
    "    start_page = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "    if kwargs.get('existing_set') == True:\n",
    "        scraped_pages_df = pd.read_csv('/home/stephbuon/projects/entascope/scraped_pages/' + ts + '/' + 'citations_' + source + '_' + ts + '.csv')\n",
    "        scraped_pages_df['url'] = scraped_pages_df['url'].apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "        scraped_pages = dict(zip(scraped_pages_df['url'], scraped_pages_df['source']))\n",
    "        \n",
    "        max_no = scraped_pages_df['article_id'].max()\n",
    "\n",
    "    else:\n",
    "        scraped_pages = {}\n",
    "        max_no = None\n",
    "\n",
    "    \n",
    "\n",
    "    #scraped_pages = import_existing_citations(source, citaitons)\n",
    "    #if scraped_pages is not None:\n",
    "    #    max_no = '' #get article ID max\n",
    "    #else:\n",
    "    #    max_no = None\n",
    "\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    article = 0\n",
    "    cycle = 0\n",
    "    for link in start_page.findAll('a'):\n",
    "\n",
    "        if 'href' in str(link):\n",
    "            page_url = link['href']\n",
    "\n",
    "            if keyword.search(page_url):\n",
    "\n",
    "                if not page_url.split('/')[-1] in scraped_pages.keys():\n",
    "\n",
    "                    ###### I can probably get rid fo recorded pages dict now that I have a method for splitting url\n",
    "                    # and then I can just record the vlidated version od scraped pages  \n",
    "                    cycle = cycle + 1\n",
    "                    if max_no is not None:\n",
    "                        if cycle == 1:\n",
    "                            article = max_no + 1\n",
    "                        else:\n",
    "                            article = article + 1\n",
    "                    else:\n",
    "                        article = article + 1\n",
    "\n",
    "                    if not 'http' in page_url and source == 'FOX':\n",
    "                        scraped_pages['https://www.foxnews.com' + page_url] = [source, article] \n",
    "                    else: \n",
    "                        scraped_pages[page_url] = [source, article]\n",
    "\n",
    "                    ### this si citations \n",
    "                  \n",
    "                \n",
    "                # make a distinction between citations and named entity extraction \n",
    "\n",
    "                    next_page = validate_url(url, page_url, source) \n",
    "\n",
    "                    di = extract_named_entities(next_page)\n",
    "                    \n",
    "                    for key in di.keys():\n",
    "                        if key in di: \n",
    "                            out.setdefault(key, []).extend(di[key])                        \n",
    "                            \n",
    "                            \n",
    "                    next_page = requests.get(page_url)\n",
    "                    if next_page is not None: # should be passed requests.get(url)\n",
    "                        try:\n",
    "                            page_content = BeautifulSoup(next_page.text, 'html.parser')\n",
    "\n",
    "                            for paragraph in page_content.findAll('p'):\n",
    "                                paragraph = list(paragraph.stripped_strings)\n",
    "                                \n",
    "                                for sentence in paragraph: \n",
    "                                    try:\n",
    "                                        sentence = nlp(sentence)\n",
    "                                        \n",
    "                                        for entity in sentence.ents:\n",
    "                                            if not entity.label_ in out.keys():\n",
    "                                                out[entity.label_] = [entity.text]\n",
    "                                            else:\n",
    "                                                out[entity.label_].append(entity.text)\n",
    "                                                \n",
    "                                    except Exception as e1:\n",
    "                                        print(e1)\n",
    "                                        \n",
    "                        except Exception as e5:\n",
    "                            print(e5)\n",
    "                          \n",
    "                    time.sleep(1)\n",
    " \n",
    "   \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #start_page.quit() # will this turn off driver?\n",
    "    export_data('/home/stephbuon/projects/entascope/data/scraped_pages/', scraped_pages, out, source, ts)\n",
    "\n",
    "# class news_scraper.dynamic_load()\n",
    "\n",
    "def scrape_pages_load_more(url, source, ts, **kwargs): # dynamic_load_scraper():\n",
    "\n",
    "    existing_set = kwargs.get('existing_set', None)\n",
    "    \n",
    "    # selenium is used to activate triggers like \"show more\" buttons\n",
    "    # the triggered website is then assigned to response (instead of using requests)\n",
    "    response = start_selenium(source, url)\n",
    "    #response = requests.get(url)\n",
    "\n",
    "    scraper(url, response, source, ts, existing_set = existing_set)\n",
    "\n",
    "\n",
    "def scrape_pages_multi_page(url, source, ts): # scrape_pages_multi_page():\n",
    "\n",
    "    keyword = re.compile(r'covid|coronavirus|vaccine|omicron|delta|vaccinat|vaxx|pandemic|mask', re.IGNORECASE)\n",
    "    \n",
    "    options = Options()\n",
    "    #options.headless = True\n",
    "    driver = webdriver.Firefox(executable_path='/home/stephbuon/projects/entascope/geckodriver', options=options)\n",
    "\n",
    "    for i in range(5): \n",
    "        if source == 'KRISTV':\n",
    "            url_w_page = url + str(i)\n",
    "\n",
    "            driver.get(url_w_page)\n",
    "            \n",
    "            try:\n",
    "                try:\n",
    "                    driver.find_element(By.CSS_SELECTOR, '.MAD_INVIEW_CLOSE').click()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", WebDriverWait(driver, .5).until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"a.List\"))))\n",
    "                WebDriverWait(driver, .5).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.List\"))).click()\n",
    "\n",
    "                scraper(driver.page_source, source, ts)\n",
    "                \n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6108/602359904.py:19: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Firefox(executable_path='/home/stephbuon/projects/entascope/geckodriver', options=options)\n"
     ]
    }
   ],
   "source": [
    "ts = time_stamp()\n",
    "scrape_pages_load_more('https://www.npr.org/sections/news/archive', 'NPR', ts)#, existing_set = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts = time_stamp()\n",
    "#scrape_pages_multi_page('https://www.kristv.com/?00000169-7dca-d05d-ab6f-7fdbf8990001-page=', 'KRISTV', ts)\n",
    "# its not under us, its under politics \n",
    "#scrape_pages('https://www.foxnews.com/politics', 'FOX', ts)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80e230aa2106b1d0405f482c44fb2bb5a0747aecf059ed39fae9051ddf6d7244"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
